% !TeX spellcheck = en_US
\chapter{Decision Stumps}
Decision stumps are the base classifiers used by AdaBoost in this project.\\
It is a tree with a node and two leaves and is defined by three parameters: an index $j$ for the feature that it cuts (in this project, the features are natural numbers), the threshold $\theta$ of the cut and the sign of the decision. A decision stump is mapped as the following function:
\begin{align}
\label{eq:dstumps_function}
	h:\mathbb{R} \to \lbrace-1, +1\rbrace
\end{align}
We can call $\mathbb{H}$ the set containing all the possible decision stumps of the form of (\ref{eq:dstumps_function}), such that $h \in \mathbb{H}$. From (\ref{eq:dstumps_function}) the set $\mathbb{H}$ can be partitioned in two subset, one for the \textit{positive stumps} and the other for the \textit{negative stumps} described formally by the following:
\begin{align}
h_{j, \theta+}(x) =
\begin{cases}
\;\;\,1       & \quad \text{if } x > \theta\\
-1  & \quad \text{otherwise}
\end{cases}
\\
h_{j,\theta-}(x) =
\begin{cases}
-1			& \quad \text{if } x \leq \theta\\
\;\;\,1		& \quad \text{otherwise}
\end{cases}
\end{align}
respectively for positives and negatives stumps.\\
Algorithm \ref{alg:dstumps} describes the implementation of a decision stump. Takes as input the training set $S$ with $d$ feature and $n$ points and a weight vector $w$. It starts with initializing the constant classifier $\gamma_{0} = 1$ which is equivalent to a positive stump with $\theta < x_{1}^{(j)}$ and sets the best edge $\gamma^{*} = \gamma_{0}$. Then it starts looping through all the $d$ features. For each feature, it will search for a better stump than $\gamma^{*}$ between non identical points. In this way, the algorithm will perform an exhaustive search looking for the best decision stump for that training set.
\\\textbf{[TODO]} describe w vector \textbf{[ENDTODO]}
\begin{algorithm}[htpb]

	\label{alg:dstumps}
	\begin{algorithmic}[1]
		\Procedure{DecisionStump}{$S$, $w$}
		\State $\gamma_{0} \gets \sum_{i=0}^{n}w_{i}y_{i}$
		\State $\gamma^{*} \gets \gamma_{0}$
		\For{$j \gets 1 \textbf{ to } d$}
			\State $\gamma \gets \gamma_{0}$
			\For{$i \gets 2 \textbf{ to }n$}
				\State $\gamma \gets \gamma - 2w_{i-1}y_{i-1}$
				\If{$x_{i-1}^{(j)} \neq x_{i}^{(j)}$}
					\If{$|\gamma| > |\gamma^{*}|$}
						\State $\gamma^{*} \gets \gamma$
						\State $j^{*} \gets j$
						\State $\theta^{*} \gets \frac{x_{i}^{(j)} + x_{i-1}^{(j)}}{2}$
					\EndIf
				\EndIf
			\EndFor
		\EndFor
		\If{$\gamma^{*} = \gamma$}{}
			\Return $h_{0}\times\text{sign}(\gamma_{0})$
		\Else{}
			\Return $h_{j^{*},\,\theta^{*}+}\times\text{sign}(\gamma^{*})$
		\EndIf
		\EndProcedure
	\end{algorithmic}
\end{algorithm}


