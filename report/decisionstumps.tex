% !TeX spellcheck = en_US

\chapter{Decision Stumps}
Decision stumps are the base classifiers used by AdaBoost in this project.\\
Those are tree predictors with a node and two leaves and are defined by three parameters: an index $j$ to indicate the feature that it cuts, the threshold $\theta$ of the cut and the sign of the decision. A decision stump has the following domain definition:
\begin{align}
\label{eq:dstumps_function}
	h:\mathbb{R}^d \to \lbrace-1, +1\rbrace
\end{align}
where $d$ is the number of features.
We call $\mathbb{H}$ the set of all  possible decision stumps. We can formally define each element $h \in \mathbb{H}$, separating the case for \textit{positive} and \textit{negative} decision stumps as:
\begin{align}
h_{j, \theta+}(x) =
\begin{cases}
\;\;\,1       & \quad \text{if } x_j > \theta\\
-1  & \quad \text{otherwise}
\end{cases}
\\
h_{j,\theta-}(x) =
\begin{cases}
-1			& \quad \text{if } x_j \leq \theta\\
\;\;\,1		& \quad \text{otherwise}
\end{cases}
\end{align}
for \textit{positive} and \textit{negative} decision stumps respectively.\\
Algorithm \ref{alg:dstumps1} describes a way to find the best decision stump given a set of weighted points. It takes in input the training set $S$ with $d$ features and $n$ points and a weight vector $w$. It starts by setting $\gamma_{0}$ with the constant classifier $\gamma_{0} = 1$ which is equivalent to a positive stump with $\theta < x_{1}^{(j)}$ and keeps the best current classifier $\gamma^{*}$ initially set as $\gamma^{*} = \gamma_{0}$. Then it starts looking through all the $d$ features. For each feature, it will search for the best decision stump, and if it's better than $\gamma^{*}$ it updates it. In this way, the algorithm will perform an exhaustive search looking for the best decision stump for that training set given those weights.
\begin{algorithm}[htpb]
\caption{}
\label{alg:dstumps1}	
	\begin{algorithmic}[1]
		\Procedure{DecisionStump}{$S$, $w$}
		\State $\gamma_{0} \gets \sum_{i=0}^{n}w_{i}y_{i}$
		\State $\gamma^{*} \gets \gamma_{0}$
		\For{$j \gets 1 \textbf{ to } d$}
			\State $\gamma \gets \gamma_{0}$
			\For{$i \gets 2 \textbf{ to }n$}
				\State $\gamma \gets \gamma - 2w_{i-1}y_{i-1}$
				\If{$x_{i-1}^{(j)} \neq x_{i}^{(j)}$}
					\If{$|\gamma| > |\gamma^{*}|$}
						\State $\gamma^{*} \gets \gamma$
						\State $j^{*} \gets j$
						\State $\theta^{*} \gets \frac{x_{i}^{(j)} + x_{i-1}^{(j)}}{2}$
					\EndIf
				\EndIf
			\EndFor
		\EndFor
		\If{$\gamma^{*} = \gamma$}{}
			\Return $h_{0}\times\text{sign}(\gamma_{0})$
		\Else{}
			\Return $h_{j^{*},\,\theta^{*}+}\times\text{sign}(\gamma^{*})$
		\EndIf
		\EndProcedure
	\end{algorithmic}
\end{algorithm}
