% !TeX spellcheck = en_US
\chapter{Introduction}
Ensemble methods are composed by a set of base classifiers, whose results are combined in order to improve the accuracy of the single classifier. In this project we've used \textit{AdaBoost}, a boosting meta-algorithm which belongs to the ensemble learning family to classify forests given a set of features such as elevation, aspect, soil types and many others. The base classifiers used by AdaBoost are the \textit{decision stumps}. A decision stump is a decision tree composed by an internal node immediately connected to the terminal nodes. In this project, the decision stumps are composed by the root and two leaves and will predict in the set $\lbrace-1, +1\rbrace$. After, we've implemented the \textit{Bagging Algorithm} to perform the same classification using \textit{one-vs-all} to perform multi-class classification with both Bagging and AdaBoost. Finally we've used the external \textit{cross-validation} to tune the hyper-parameter $T$ of both Bagging and AdaBoost and evaluate the accuracy of the algorithms. \\
This document will start with an abstract description of all the tools that have been used and a final chapter regarding this project results.\\
This project has been implemented in C++ and the whole code can be found on GitHub\footnote{https://github.com/dariost/AdaBoost}. 