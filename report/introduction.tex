% !TeX spellcheck = en_US
\chapter{Introduction}
Ensemble methods use a set of multiple base classifiers to obtain results that are better than the ones provided by the single classifiers. In this project we've used \textit{AdaBoost}, a boosting meta-algorithm which belongs to the ensemble learning family, to classify forests given a set of features such as elevation, aspect, soil types and many others. The base classifiers used by AdaBoost are \textit{decision stumps}. A decision stump is a decision tree composed of a single internal node immediately connected to the terminal nodes. In this project, the decision stumps are composed of the root and two leaves and will predict in the set $\lbrace-1, +1\rbrace$. Afterwards, we've implemented the \textit{Bagging Algorithm} to perform the same classification. We've used \textit{one-vs-all} to perform multi-class classification with both Bagging and AdaBoost. Finally we've used external \textit{cross-validation} to tune the hyper-parameter $T$ of both Bagging and AdaBoost and evaluate the accuracy of the algorithms.

This report will start with an abstract description of all the tools that have been used and end with a final chapter regarding this project results.\\
This project has been implemented in C++ and the whole code-base can be found on GitHub\footnote{https://github.com/dariost/AdaBoost} as well as two utilities written in Python, one for result plotting from \texttt{json} files and the other one for parsing the data-set in \texttt{csv} format and convert it in another format that is easier to parse in C++. The final data-set is then of the form:
\begin{center}
\begin{tabular}{ccccccccc}
	15120 & 54 & 7 &  &  &  &  &  &  \\ 
	1 & 2 & 3 & 4 & 5 & 6 & 7 &  &  \\ 
	5 & 2596 & 51 & 3 & 258 & 0 & 510 & 221 & ... \\
	5 & 2529 & 56 & 2 & 212 & -6 & 390 & 220 & ... \\
	2 & 2804 & 139 & 9 & 268 & 65 & 3180 & 234 & ... \\
	... & ... & ... & ... & ... & ... & ... & ... & ... \\
\end{tabular}
\end{center} 
where the first three number are respectively the sample size (e.g. number of rows), the number of features (e.g. the number of columns) and the number of labels; in the second row, there are all the labels imported from the original data-set and finally all the samples. In this project, the data-set in input contains only real number, hence for simplicity all the entries are evaluated and treated as \texttt{double}s.\\
The program start by importing the parsed data-set, saved with the format \textit{ab} (for no particular reasons). It then starts the training of both AdaBoost and Bagging with the One-Vs-All multi-class technique, due to the fact that this is not a binary problem. Those two instances are encapsulated in two different cross-validation instances. An unlimited \textit{for-loop} is then launched. Each cycle of this loop has been called an ``epoch'', and the iteration number is the hyper-parameter given to both Bagging and AdaBoost.
