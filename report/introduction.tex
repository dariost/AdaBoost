% !TeX spellcheck = en_US
\chapter{Introduction}
Ensemble methods are composed by a set of base classifiers, whose results are combined in order to improve the accuracy of the single classifier. In this project we've used \textit{AdaBoost}, a boosting meta-algorithm which belongs to the ensemble learning family to classify forests given a set of features such as elevation, aspect, soil types and many others. The base classifiers used by AdaBoost are the \textit{decision stumps}. A decision stump is a decision tree composed by an internal node immediately connected to the terminal nodes. In this project, the decision stumps are composed by the root and two leaves and will predict in the set $\lbrace-1, +1\rbrace$. After, we've implemented the \textit{Bagging Algorithm} to perform the same classification using \textit{one-vs-all} to perform multi-class classification with both Bagging and AdaBoost. Finally we've used the external \textit{cross-validation} to tune the hyper-parameter $T$ of both Bagging and AdaBoost and evaluate the accuracy of the algorithms. \\
This document will start with an abstract description of all the tools that have been used and a final chapter regarding this project results.\\
This project has been implemented in C++ and the whole code can be found on GitHub\footnote{https://github.com/dariost/AdaBoost} as long with two utilities Python scripts, one for result plotting from \texttt{json} files and one for parsing the data-set in \texttt{csv} format to rebuild it in a more \textit{C++-like} style. The final data-set is a file of the form:
\begin{center}
\begin{tabular}{ccccccccc}
	15120 & 54 & 7 &  &  &  &  &  &  \\ 
	1 & 2 & 3 & 4 & 5 & 6 & 7 &  &  \\ 
	5 & 2596 & 51 & 3 & 258 & 0 & 510 & 221 & ... \\
	5 & 2529 & 56 & 2 & 212 & -6 & 390 & 220 & ... \\
	2 & 2804 & 139 & 9 & 268 & 65 & 3180 & 234 & ... \\
	... & ... & ... & ... & ... & ... & ... & ... & ... \\
\end{tabular}
\end{center} 
where the first three number are respectively the sample size (e.g. number of rows), the number of features (e.g. the number of columns) and the number of labels; in the second row, there are all the labels imported from the original data-set and finally all the samples. In this project, the data-set in input contains only real number, hence for simplicity all the entries are evaluated and treated as \texttt{double}s.\\
The program start by importing the parsed data-set, saved with the format \textit{ab} (for no particular reasons). It then starts the training of both AdaBoost and Bagging with the One-Vs-All multi-class technique, due to the fact that this is not a binary problem. Those two instances are encapsulated in two different cross-validation instances. An unlimited \textit{for-loop} is then launched. Each cycle of this loop has been called an ``epoch'', and the iteration number is the hyper-parameter given to both Bagging and AdaBoost.
