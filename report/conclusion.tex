% !TeX spellcheck = en_US
\chapter{Conclusion}
We've chosen to implement from scratch all the algorithms of this project using \CC17 for better performance and using Python3 for utilities, like plotting or to prepare the dataset to be better parsed with \CC. The dataset provided was about about forest cover type classification for seven types of trees. The main objective was to compare two ensemble methods: AdaBoost and Bagging, using as base classifier Decision Stumps and evaluating test error with $K$-fold external cross validation.

Our algorithm performs an iteration to execute cross validation of both AdaBoost and Bagging for any $t$ epoch, executing the loss function calculation at each $t$. The power of the OpenMP and the cluster used to execute our project let us provide many measurements for the best number of iteration that AdaBoost has to execute to perform the lower training error whilst avoiding overfitting. Those results are reported in \autoref{chap:results}, \autoref{tab:results}. On the other hand, even executing thousands of epoch (in some cases, over $40000$ epochs) we couldn't be able to proide the best T value for the Bagging algorithm. Moreover, at any epoch $t$, the Bagging algorithm has a training and test error way higher than the errors of AdaBoost, as we've seen in \autoref{chap:results}.