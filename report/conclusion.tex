% !TeX spellcheck = en_US
\chapter{Conclusion}
We've chosen to implement from scratch all the algorithms of this project using \CC17 for better performance and using Python3 for utilities, like plotting or to prepare the dataset to be better parsed with \CC. The dataset provided was about forest cover type classification for seven types of trees. The main objective was to compare two ensemble methods: AdaBoost and Bagging, using as base classifier Decision Stumps and evaluating test error with $K$-fold external cross validation.

The results presented in chapter \ref{chap:results} show that, given the same time to train with both algorithms (since both use Decision Stumps as base classifiers), AdaBoost gives a much better predictor that Boosting for any $T > m$ for some very small $m$. They also show that, for this dataset, is useless training AdaBoost for more than $1000$ epochs, because for all the parameters ok $K$ tried, it always causes overfitting to occur.

The last thing to notice is that even with over $40000$ epochs, Bagging would not start to overfit. In order to reach the best $T$ value for Bagging we would need to have more computational time, more computational power or a better implementation (for example, we could implement the algorithm to find the best Decision Stump so that it performs this search using a GPU).
