% !TeX spellcheck = en_US
\chapter{Conclusion}
We have chosen to implement from scratch all the algorithms of this project with \CC17 for better performance and using Python3 for utilities, like plotting or prepare the dataset to be better parsed with \CC. The dataset provided was questioning about forest cover type classification for seven types of trees. The main objective was to compare two ensemble methods meta-algorithms: AdaBoost and Bagging, using as base classifier the Decision Stumps and evaluating the test error with the K-fold cross validation. Our main algorithm perform an iteration to execute cross validation of both AdaBoost and Bagging for any $t$ epoch, executing the loss function calculation at each $t$. The power of the OpenMP and the cluster used to execute our project let us provide many measurements for the best number of iteration that AdaBoost has to execute to perform the lower training error whilst avoiding overfitting. Those results are reported in \autoref{chap:results}, \autoref{tab:results}. On the other hand, even executing thousands of epoch (in some cases, over $40000$ epochs) we couldn't be able to proide the best T value for the Bagging algorithm. Moreover, at any epoch $t$, the Bagging algorithm has a training and test error way higher than the errors of AdaBoost, as we've seen in \autoref{chap:results}.