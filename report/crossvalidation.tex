% !TeX spellcheck = en_US

\chapter{Cross-Validation}
Cross-validation is a model validation technique mainly used in prediction models. Its purpose is parameter tuning of parametric learning algorithms such as the $T$ parameter of \textit{AdaBoost}. It can also be used to estimate the accuracy of a given family of algorithms $A$. In this project, we used the \textit{external cross-validation} - also called \textit{K-fold} cross-validation - to evaluate both AdaBoost and \textit{The Bagging Algorithm} by estimating the expected value of the statistical risk.
\begin{align*}
	\mathbb{E}[\ell_{\mathcal{D}}(A)]
\end{align*}
Where $\mathcal{D}$ is the data distribution and $A$ is the learning algorithm. Given a data-set
\begin{align*}
	S = \lbrace (x_{1},y_{y}),\;\dots\;,(x_{m},y_{m}) \rbrace
\end{align*}
we have to partition $\mathcal{D}$ in $K$ subsets, or ``folds'' of size $m/K$ each. For each of the $\mathcal{D}_{k}$ partition, we want to take one as the test set (also called validation set) and let all the others be the training set. At each $k$ cycle, the algorithm $A$ will be training on the subsets chosen to be the training set and then will be evaluated over the subset chosen to be the test set. Finally, the results of all the evaluation can be averaged to have an average loss of $A$ over $\mathcal{D}$.

\begin{algorithm}[]
	\caption{}
	\label{alg:crossvalidation}
	\begin{algorithmic}[1]
		\Procedure{CrossValidation}{$L(\cdot,\cdot), \mathcal{D}, K, T$}
		\State $\ell_{S} \gets 0$
		\State $\ell_{\mathcal{D}} \gets 0$
		\For{$k=1 \textbf{ to } K$}
			\State $size \gets \min(\frac{m}{K}, |\mathcal{D}|)$
			\State $\mathcal{D}_{k} \equiv \lbrace d_{i} \;|\; d_{i} \in \mathcal{D} \:\;\;0 \leq i \leq size\rbrace$
			\State $\mathcal{D} \gets \mathcal{D}\setminus\mathcal{D}_{k}$
		\EndFor
		\For{$k=1 \textbf{ to } K$}
			\State $S_{k} \gets \mathcal{D}_{k}$
			\State $V_{k} \gets \mathcal{D}_{k}\setminus S_{k}$
			\State $\ell_{S} \gets \ell_{S} + \ell(L(S_{k}, T))$
			\State $\ell_{\mathcal{D}} \gets \ell_{\mathcal{D}} + \ell(L(V_{k}, T)) $
		\EndFor
		\Return $(\frac{\ell_{S}}{K},\frac{\ell_{\mathcal{D}}}{K})$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:crossvalidation} describes in pseudo-code a possible implementation of a $K$-fold cross-validation that returns the average error on the training set and the validation set respectively. It takes in input a learner $L$, a dataset $\mathcal{D}$, a parameter $K$ for the number of folds to perform and a hyper-parameter $T$ which has to be tuned. This procedure is repeated a certain amount of times to really catch the best value for $T$ which minimize the algorithm's loss function. In our scenario we have set $K=20$ and $T$ is the hyper-parameter for both AdaBoost and Bagging.