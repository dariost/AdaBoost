% !TeX spellcheck = en_US

\chapter{AdaBoost}
\section{General}
	The AdaBoost learning meta-algorithm combines the results of a sequence of basic classifiers with a weighted sum into a predictor $f$. \\
	Let the training set be $S=\lbrace (x_{1}, y_{1}),\; \dots\;, (x_{m}, y_{m}) \rbrace$, and be $T$ the hyper-parameter which tells AdaBoost how many iteration must run to perform the prediction. $T$ can be set, for example, through cross validation. Given a base predictor $h_{i} \in \mathbb{H}\;$, which is the set of the base predictors, for each iteration $i = 1,\;\dots\;,T$ we set a coefficient $\alpha_{i}$. The base classifier $h_{i}$ is a simple binary classifier of the form $h:\mathbb{R}^{d}\to\lbrace-1, 1\rbrace$; the  predictor $f$ is then formally described as follow:
	\begin{align}
	\label{eq:weighted_sum}
		f(x) = \sum_{t=1}^{T}\alpha_{t}h_{t}(x)
	\end{align}
	Where $\alpha_{1},\;\dots\;,\alpha_{T}$ are the weight coefficients and are set to be
	\begin{align*}
		\alpha_{t} = \frac{1}{2}\ln \Big( \frac{1-\varepsilon{t}}{\varepsilon_{t}} \Big) \;\;\;\; \forall t=1,\;\dots\;,T
	\end{align*}
	and $\varepsilon_{t}$ is the weighted error of $h_{t}$. Each weighted error is given by the weighted sum
	\begin{align*}
		\varepsilon_{t} = \sum_{i=1}^{n} w_{i}^{(t)}\mathbb{I}\big\lbrace h^{(t)}(x_{i}) \neq y_{i} \big\rbrace
	\end{align*}
	Where $w_{i}$ for $i = 1,\;\dots\;, n$ is a weight distribution over the data points, initially set uniformly and updated successively in each iteration.\\\\
	This value of $\alpha_{t}$ is chosen due to the fact that it is the [TODO]zero derivative with respect to the weights $\alpha_{t}$ of the training error's upper bound of AdaBoost $\hat{\ell}_{S}(f)$, which is
	\begin{align*}
		\hat{\ell}_{S}(f) \leq \prod_{t=1}^{T}\big( e^{-\alpha_{t}}(1-\varepsilon_{t}) + e^{\alpha_{t}}\varepsilon_{t} \big)
	\end{align*}
	[ENDTODO]
	
%\begin{align}
%	\mathbb{P} (h_{1}(x_{Z}) \neq y_{Z} \land \dots \land h_{T}(x_{Z}) \neq y_{Z}) = %\prod_{i=1}^{T}w_{i}h_{i}
%\end{align}
%\begin{align}
%	\hat{\ell}(f) \leq \exp(-2\sum_{i=1}^{T}\gamma^{2}_{i})
%\end{align}
\section{Implementation}
The implementation code for AdaBoost can be found in the \texttt{AdaBoost} class and it is defined by five attributes:
\begin{itemize}
	\item \texttt{dataset} is a view on (possibly a subset of) the data set;
	\item \texttt{w} represents the $w$ vector of the weights distribution over data points;
	\item \texttt{a} is a vector used to weight the influence of the single predictor;
	\item \texttt{h} represents the set of the base classifiers; and
	\item \texttt{label} is the label that the predictor will predict as $+1$; all other labels will predicted as $-1$.
\end{itemize}
The main algorithm is described in pseudo-code by algorithm (\ref{alg:adaboost}) and its main for loop is implemented by the method \texttt{next\_epoch()}. Each iteration is handled by the \texttt{OneVsAll} class.
\begin{algorithm}[htpb]
	\caption{}
	\label{alg:adaboost}
	\begin{algorithmic}[]
		\Procedure{AdaBoost}{$S$, $BASE(\cdot,\cdot)$, $T$}
		\State $w^{(0)} \gets (1/n,\;\dots\;,1/n)$
		\For{$t \gets 1 \textbf{ to } T$}
			\State $h^{(t)} \gets BASE(S, w^{(t)})$
			\State $\varepsilon{t} \gets \sum_{i=1}^{n}w_{i}^{(t)}\mathbb{I}\lbrace h^{(t)}(x_{i} \neq y_{i}) \rbrace $
			\State $\alpha^{(t)} \gets \frac{1}{2}\ln\Big(\frac{1-\varepsilon_{t}}{\varepsilon_{t}}\Big)$
			\For{$i\gets1\textbf{ to } n$}
				\If{$\mathbb{I}\lbrace h^{(t)}(x_{i} \neq y_{i}\rbrace$}
					\State $w_{i}^{(t+1)}\gets\frac{w_{i}^{(t)}}{2\varepsilon{(t)}}$
				\Else
					\State $w_{i}^{(t+1)}\gets\frac{w_{i}^{(t)}}{2(1-\varepsilon{(t)})}$
				\EndIf
			\EndFor
		\EndFor
		\Return $\sum_{t=1}^{T}\alpha^{(t)}h^{(t)}$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}






