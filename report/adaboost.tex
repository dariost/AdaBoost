% !TeX spellcheck = en_US

\chapter{AdaBoost}
\section{General}
\label{sec:adaboost_general}
	The AdaBoost learning meta-algorithm combines the results of a sequence of basic classifiers with a weighted sum into a predictor $f$. \\
	Let the training set be $S=\lbrace (x_{1}, y_{1}),\; \dots\;, (x_{m}, y_{m}) \rbrace$, and be $T$ the hyper-parameter which tells AdaBoost how many iteration must run to perform the prediction. $T$ can be set, for example, through cross validation. Given a base predictor $h_{i} \in \mathbb{H}\;$, which is the set of the base predictors, for each iteration $i = 1,\;\dots\;,T$ we set a coefficient $\alpha_{i}$. The base classifier $h_{i}$ is a simple binary classifier of the form $h:\mathbb{R}^{d}\to\lbrace-1, 1\rbrace$; the  predictor $f$ is then formally described as follow:
	\begin{align*}
		f(x) = \sum_{t=1}^{T}\alpha_{t}h_{t}(x)
	\end{align*}
	Where $\alpha_{1},\;\dots\;,\alpha_{T}$ are the weight coefficients and are set to be
	\begin{align}
		\label{eq:alpha_def}
		\alpha_{t} = \frac{1}{2}\ln \Big( \frac{1-\varepsilon{t}}{\varepsilon_{t}} \Big) \;\;\;\; \forall t=1,\;\dots\;,T
	\end{align}
	and $\varepsilon_{t}$ is the weighted error of $h_{t}$. Each weighted error is given by the weighted sum
	\begin{align*}
		\varepsilon_{t} = \sum_{i=1}^{n} w_{i}^{(t)}\mathbb{I}\big\lbrace h^{(t)}(x_{i}) \neq y_{i} \big\rbrace
	\end{align*}
	Where $w_{i}$ for $i = 1,\;\dots\;, n$ is a weight distribution over the data points, initially set uniformly and updated successively in each iteration.\\\\
	This value of $\alpha_{t}$ is chosen due to the fact that it is the [\textbf{TODO}]zero derivative with respect to the weights $\alpha_{t}$ of the training error's upper bound of AdaBoost $\hat{\ell}_{S}(f)$, which is
	\begin{align*}
		\hat{\ell}_{S}(f) \leq \prod_{t=1}^{T}\big( e^{-\alpha_{t}}(1-\varepsilon_{t}) + e^{\alpha_{t}}\varepsilon_{t} \big)
	\end{align*}
	[\textbf{ENDTODO}]

The algorithm that has been implemented of AdaBoost is described by (\ref{alg:adaboost}) and takes three parameters:
\begin{itemize}
	\item $S$ is the training set;
	\item $BASE(S,\;w^{t})$ is the base classifier used by AdaBoost, it takes the training set and the $w^{t}$ vector of weights at each \textit{t-th} iteration; and
	\item $T$ is the number of iteration that AdaBoost has to run before return its result.
\end{itemize}
\begin{algorithm}[htpb]
	\caption{}
	\label{alg:adaboost}
	\begin{algorithmic}[1]
		\Procedure{AdaBoost}{$S$, $BASE(\cdot,\cdot)$, $T$}
		\State $w^{(0)} \gets (1/n,\;\dots\;,1/n)$ \label{instr:w_init}
		\For{$t \gets 1 \textbf{ to } T$}
			\State $h^{(t)} \gets BASE(S, w^{(t)})$
			\State $\varepsilon^{(t)} \gets \sum_{i=1}^{n}w_{i}^{(t)}\mathbb{I}\lbrace h^{(t)}(x_{i} \neq y_{i}) \rbrace $
			\State $\alpha^{(t)} \gets \frac{1}{2}\ln\Big(\frac{1-\varepsilon^{(t)}}{\varepsilon^{(t)}}\Big)$
			\For{$i\gets1\textbf{ to } n$}
				\If{$\mathbb{I}\lbrace h^{(t)}(x_{i} \neq y_{i})\rbrace$}
					\State $w_{i}^{(t+1)}\gets\frac{w_{i}^{(t)}}{2\varepsilon^{(t)}}$
				\Else
					\State $w_{i}^{(t+1)}\gets\frac{w_{i}^{(t)}}{2(1-\varepsilon^{(t)})}$
				\EndIf
			\EndFor
		\EndFor
		\Return $\sum_{t=1}^{T}\alpha^{(t)}h^{(t)}$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}
It starts with initializing the weight vector to a uniform value and updates it at each iteration based on the prediction made by the base classifier, stored in $h^{(t)}$. The new values of $w^{(t)}$ are studied to maintain the whole vector normalized such that
\begin{align}
	\label{eq:w_norm}
	\sum_{i=1}^{n}w^{(t)}_{i} = 1
\end{align}
In this way we can properly use the expression (\ref{eq:alpha_def}) which is defined for $0 < \varepsilon^{(t)} < 1$.
To proof that, assuming that line \ref{instr:w_init} of the algorithm (\ref{alg:adaboost}) maintains expression (\ref{eq:w_norm}):
\begin{equation*}
\begin{split}
	\sum_{i=1}^{n}w_{i}^{(t+1)} & = \\
	& = \sum_{i=1}^{n}\frac{w_{i}^{(t)}}{2\varepsilon^{(t)}}\mathbb{I}
		\lbrace 
			h^{(t)}(x_{i}) \neq y_{i}
		\rbrace 
	+
	\sum_{i=1}^{n}\frac{w_{i}^{(t)}}{2(1-\varepsilon^{(t)})}\mathbb{I}
	\lbrace
		h^{(t)}(x_{i}) = y_{i}
	\rbrace = \\ 
	& = \frac{1}{2\varepsilon^{(t)}}\sum_{i=1}^{n}w_{i}^{(t)}\mathbb{I}
	\lbrace 
		h^{(t)}(x_{i}) \neq y_{i} 
	\rbrace 
	+ 
	\frac{1}{2(1-\varepsilon^{(t)})}\sum_{i=1}^{n}w_{i}^{(t)}
	\big(
		1-\mathbb{I} \lbrace
			{h^{(t)}}(x_{i}) \neq y_{i}
		\rbrace
	\big) = \\ 
	& = \frac{1}{2\varepsilon^{(t)}}\varepsilon^{(t)}+\frac{1}{2(1-\varepsilon^{(t)})}
	(
		1-\varepsilon^{(t)}
	) =
	\frac{1}{2} + \frac{1}{2} = 1
\end{split}
\end{equation*}





