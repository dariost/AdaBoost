% !TeX spellcheck = en_US

\chapter{AdaBoost}
\section{General}
	The AdaBoost learning meta-algorithm $f$ combines the results of a sequence of basic classifiers with a weighted sum. \\
	Let the training set be $S=\lbrace (x_{1}, y_{1}),\; \dots\;, (x_{m}, y_{m}) \rbrace$, and be $T$ the hyper-parameter which tells AdaBoost how many iteration must run to perform the prediction. $T$ can be set, for example, through cross validation. Given a base predictor $h_{i} \in \mathbb{H}\;$ for each iteration $i = 1,\;\dots\;T$ we set a coefficient $\alpha_{i}$. The base classifiers $h_{i}$ is a simple binary classifier of the form $h:\mathbb{R}^{d}\to\lbrace-1, 1\rbrace$, the base learner is formally described as follow:
	\begin{align*}
		f(x) = \sum_{i=1}^{T}\alpha_{i}h_{i}(x)
	\end{align*}
	Where $\alpha_{1},\;\dots\;,\alpha_{T}$ are the weight coefficients and are set to be
	\begin{align*}
		\alpha_{t} = \frac{1}{2}\ln \Big( \frac{1-\epsilon_{t}}{\epsilon_{t}} \Big) \;\;\;\; \forall t=1,\;\dots\;,T
	\end{align*}
	and $\epsilon_{t}$ is the \textit{t-esim} is the weighted error of $h_{t}$ with respect to the probability $\mathbb{P}_{t}$. The chosen value for $\alpha_{t}$ is due to the fact that it is the zero derivative with respect to the weights $\alpha_{t}$ of the training error's upper bound of AdaBoost $\hat{\ell}_{S}(f)$ on the training set $S$, which is
	\begin{align*}
		\hat{\ell}_{S} \leq \prod_{i=1}^{T}\big( e^{-\alpha_{i}}(1-\epsilon_{i}) + e^{\alpha_{i}}\epsilon_{i} \big)
	\end{align*}
	The learner output is the sign of $f$.
	
%\begin{align}
%	\mathbb{P} (h_{1}(x_{Z}) \neq y_{Z} \land \dots \land h_{T}(x_{Z}) \neq y_{Z}) = %\prod_{i=1}^{T}w_{i}h_{i}
%\end{align}
%\begin{align}
%	\hat{\ell}(f) \leq \exp(-2\sum_{i=1}^{T}\gamma^{2}_{i})
%\end{align}
\section{Implementation}