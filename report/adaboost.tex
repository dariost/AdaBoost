% !TeX spellcheck = en_US

\chapter{AdaBoost}
\section{General}
	The AdaBoost learning meta-algorithm $f$ combines the results of a sequence of basic classifiers with a weighted sum. \\
	Let the training set be $S=\lbrace (x_{1}, y_{1}),\; \dots\;, (x_{m}, y_{m}) \rbrace$, and be $T$ the hyper-parameter which tells AdaBoost how many iteration must run to perform the prediction. $T$ can be set, for example, through cross validation. Given a base predictor $h_{i} \in \mathbb{H}\;$, which is the set of the base predictors, for each iteration $i = 1,\;\dots\;T$ we set a coefficient $\alpha_{i}$. The base classifiers $h_{i}$ is a simple binary classifier of the form $h:\mathbb{R}^{d}\to\lbrace-1, 1\rbrace$, the base learner is formally described as follow:
	\begin{align*}
		f(x) = \sum_{i=1}^{T}\alpha_{i}h_{i}(x)
	\end{align*}
	Where $\alpha_{1},\;\dots\;,\alpha_{T}$ are the weight coefficients and are set to be
	\begin{align*}
		\alpha_{t} = \frac{1}{2}\ln \Big( \frac{1-\epsilon_{t}}{\epsilon_{t}} \Big) \;\;\;\; \forall t=1,\;\dots\;,T
	\end{align*}
	and $\epsilon_{t}$ is the \textit{t-esim} weighted error of $h_{t}$. Each weighted error is given by the weighted sum
	\begin{align*}
		\epsilon_{t} = \sum_{i=1}^{n} w_{i}^{(t)}\mathbb{I}\big\lbrace h^{(t)}(x_{i}) \neq y_{i} \big\rbrace
	\end{align*}
	Where $w_{i}$ for $i = 1,\;\dots\;, n$ is a weight distribution over the data points, initially set uniformly and updated successively in each iteration.\\\\
	The chosen value for $\alpha_{t}$ is due to the fact that it is the zero derivative with respect to the weights $\alpha_{t}$ of the training error's upper bound of AdaBoost $\hat{\ell}_{S}(f)$ on the training set $S$, which is
	\begin{align*}
		\hat{\ell}_{S}(f) \leq \prod_{i=1}^{T}\big( e^{-\alpha_{i}}(1-\epsilon_{i}) + e^{\alpha_{i}}\epsilon_{i} \big)
	\end{align*}
	
	
%\begin{align}
%	\mathbb{P} (h_{1}(x_{Z}) \neq y_{Z} \land \dots \land h_{T}(x_{Z}) \neq y_{Z}) = %\prod_{i=1}^{T}w_{i}h_{i}
%\end{align}
%\begin{align}
%	\hat{\ell}(f) \leq \exp(-2\sum_{i=1}^{T}\gamma^{2}_{i})
%\end{align}
\section{Implementation}
\begin{algorithm}[htpb]
	\label{alg:adaboost}
	\begin{algorithmic}[]
		\Procedure{AdaBoost}{$S$, $BASE(\cdot,\cdot)$, $T$}
		\State $w^{(0)} \gets (1/n,\;\dots\;,1/n)$
		\For{$t \gets 1 \textbf{ to } T$}
			\State $h_{t} \gets BASE(S, w^{(t)})$
			\State $\epsilon_{t} \gets \sum_{i=1}^{n}w_{i}^{(t)}\mathbb{I}\lbrace h^{(t)}(x_{i} \neq y_{i}) \rbrace $
			\State $\alpha^{(t)} \gets \frac{1}{2}\ln\Big(\frac{1-\epsilon_{t}}{\epsilon_{t}}\Big)$
			\For{$i\gets1\textbf{ to } n$}
				\If{$\mathbb{I}\lbrace h^{(t)}(x_{i} \neq y_{i}$}
					\State $w_{i}^{(t+1)}\gets\frac{w_{i}^{(t)}}{2\epsilon{(t)}}$
				\Else
					\State $w_{i}^{(t+1)}\gets\frac{w_{i}^{(t)}}{2(1-\epsilon{(t)})}$
				\EndIf
			\EndFor
		\EndFor
		\Return $\sum_{t=1}^{T}\alpha^{(t)}h^{(t)}$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}






