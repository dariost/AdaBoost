% !TeX spellcheck = en_US

\chapter{AdaBoost}
\section{General}
	The AdaBoost learning meta-algorithm $f$ combines the results of a sequence of basic classifiers with a weighted sum. \\
	Let the training set be $S=\lbrace (x_{1}, y_{1}),\; \dots\;, (x_{m}, y_{m}) \rbrace$, and be $T$ the hyper-parameter which tells AdaBoost how many iteration must run to perform the prediction. $T$ can be set, for example, through cross validation. Given a base predictor $h_{i} \in \mathbb{H}\;$ for each iteration $i = 1,\;\dots\;T$ we set a coefficient $\alpha_{i}$. The base classifiers $h_{i}$ is a simple binary classifier of the form $h:\mathbb{R}^{d}\to\lbrace-1, 1\rbrace$. The output of AdaBoost is sign of the function below:
	\begin{align*}
		f(\cdot) = \sum_{i=1}^{T}\alpha_{i}h_{i}(\cdot)
	\end{align*}
	The learner output is the sign of $f(\cdot)$. 
%\begin{align}
%	\mathbb{P} (h_{1}(x_{Z}) \neq y_{Z} \land \dots \land h_{T}(x_{Z}) \neq y_{Z}) = %\prod_{i=1}^{T}w_{i}h_{i}
%\end{align}
%\begin{align}
%	\hat{\ell}(f) \leq \exp(-2\sum_{i=1}^{T}\gamma^{2}_{i})
%\end{align}
\section{Implementation}